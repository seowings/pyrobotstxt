{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pyrobotstxt - A Python Package for robots.txt Files \u00b6 pyrobotstxt package can be used to (systematically) generate robots.txt files. Moreover, this package comes in handy for creating and including ASCII images in robots.txt files. In future releases, it would be possible to parse, analyze and manipulate robots.txt file generated using any software (not limited to pyrobotstxt ) Whats in pyrobotstxt? \u00b6 We believe in monolithic software development and created this tiny package that does its job without any bloat. It is useful for Createing robots.txt File Parsing a robots.txt File [in progress] Analyzing robots.txt File [in progress] How to Use pyrobotstxt? \u00b6 You can follow our basic user tutorial on how to use this library. If you are a developer or want to test latest version of pyrobotstxt, you might find information on our developers section useful. About Us \u00b6 SERP Wings is a digital organization which develops software solutions to Boosting SERP Performance Though Opensource Tools . seowings is an opensource project to write, develop and promote tools for Data Sciences and Digital Marketing.","title":"Home"},{"location":"#pyrobotstxt-a-python-package-for-robotstxt-files","text":"pyrobotstxt package can be used to (systematically) generate robots.txt files. Moreover, this package comes in handy for creating and including ASCII images in robots.txt files. In future releases, it would be possible to parse, analyze and manipulate robots.txt file generated using any software (not limited to pyrobotstxt )","title":"pyrobotstxt - A Python Package for robots.txt Files"},{"location":"#whats-in-pyrobotstxt","text":"We believe in monolithic software development and created this tiny package that does its job without any bloat. It is useful for Createing robots.txt File Parsing a robots.txt File [in progress] Analyzing robots.txt File [in progress]","title":"Whats in pyrobotstxt?"},{"location":"#how-to-use-pyrobotstxt","text":"You can follow our basic user tutorial on how to use this library. If you are a developer or want to test latest version of pyrobotstxt, you might find information on our developers section useful.","title":"How to Use pyrobotstxt?"},{"location":"#about-us","text":"SERP Wings is a digital organization which develops software solutions to Boosting SERP Performance Though Opensource Tools . seowings is an opensource project to write, develop and promote tools for Data Sciences and Digital Marketing.","title":"About Us"},{"location":"developers/","text":"Developers Tutorial \u00b6 A developer tutorial is available at serpwings pyrobotstxt tutorial page","title":"Developer Tutorial"},{"location":"developers/#developers-tutorial","text":"A developer tutorial is available at serpwings pyrobotstxt tutorial page","title":"Developers Tutorial"},{"location":"imageasascii/","text":"ImageAsASCII Class \u00b6 Class to Convert RGB/GRAYSCALE Images to ASCII (Text) format. Source code in pyrobotstxt/__init__.py class ImageAsASCII: \"\"\"Class to Convert RGB/GRAYSCALE Images to ASCII (Text) format.\"\"\" def __init__(self, image_path=None, desired_width=90): \"\"\"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Args: desired_width (int, optional): width of the desired output ASCII Image. image_path (str, optional):path of the input image. If None then conversion will not work. \"\"\" if not image_path: raise ValueError self.ascii_str_map = [\" \", *(\"*$+?.%;:,@\")] self.ascii_image = \"\" self.image = Image.open(image_path).convert(\"L\") desired_height = desired_width * self.image.height / self.image.width self.image = self.image.resize((ceil(desired_width), ceil(desired_height))) def map_to_ascii(self): \"\"\"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. \"\"\" str_container = \"\" # a container to hold ascii charcters for pixel in self.image.getdata(): str_container += self.ascii_str_map[pixel // 25] self.ascii_image = \"#\\t\" # Now transform the string container to column format. for i in range(0, len(str_container), self.image.width): self.ascii_image += ( \" \".join(str_container[i : i + self.image.width]) + \"\\n#\\t\" ) __init__(image_path=None, desired_width=90) \u00b6 intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Parameters: Name Type Description Default desired_width int width of the desired output ASCII Image. 90 image_path str path of the input image. If None then conversion will not work. None Source code in pyrobotstxt/__init__.py def __init__(self, image_path=None, desired_width=90): \"\"\"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Args: desired_width (int, optional): width of the desired output ASCII Image. image_path (str, optional):path of the input image. If None then conversion will not work. \"\"\" if not image_path: raise ValueError self.ascii_str_map = [\" \", *(\"*$+?.%;:,@\")] self.ascii_image = \"\" self.image = Image.open(image_path).convert(\"L\") desired_height = desired_width * self.image.height / self.image.width self.image = self.image.resize((ceil(desired_width), ceil(desired_height))) map_to_ascii() \u00b6 map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. Source code in pyrobotstxt/__init__.py def map_to_ascii(self): \"\"\"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. \"\"\" str_container = \"\" # a container to hold ascii charcters for pixel in self.image.getdata(): str_container += self.ascii_str_map[pixel // 25] self.ascii_image = \"#\\t\" # Now transform the string container to column format. for i in range(0, len(str_container), self.image.width): self.ascii_image += ( \" \".join(str_container[i : i + self.image.width]) + \"\\n#\\t\" )","title":"ImageAsASCII"},{"location":"imageasascii/#imageasascii-class","text":"Class to Convert RGB/GRAYSCALE Images to ASCII (Text) format. Source code in pyrobotstxt/__init__.py class ImageAsASCII: \"\"\"Class to Convert RGB/GRAYSCALE Images to ASCII (Text) format.\"\"\" def __init__(self, image_path=None, desired_width=90): \"\"\"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Args: desired_width (int, optional): width of the desired output ASCII Image. image_path (str, optional):path of the input image. If None then conversion will not work. \"\"\" if not image_path: raise ValueError self.ascii_str_map = [\" \", *(\"*$+?.%;:,@\")] self.ascii_image = \"\" self.image = Image.open(image_path).convert(\"L\") desired_height = desired_width * self.image.height / self.image.width self.image = self.image.resize((ceil(desired_width), ceil(desired_height))) def map_to_ascii(self): \"\"\"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. \"\"\" str_container = \"\" # a container to hold ascii charcters for pixel in self.image.getdata(): str_container += self.ascii_str_map[pixel // 25] self.ascii_image = \"#\\t\" # Now transform the string container to column format. for i in range(0, len(str_container), self.image.width): self.ascii_image += ( \" \".join(str_container[i : i + self.image.width]) + \"\\n#\\t\" )","title":"ImageAsASCII Class"},{"location":"imageasascii/#pyrobotstxt.ImageAsASCII.__init__","text":"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Parameters: Name Type Description Default desired_width int width of the desired output ASCII Image. 90 image_path str path of the input image. If None then conversion will not work. None Source code in pyrobotstxt/__init__.py def __init__(self, image_path=None, desired_width=90): \"\"\"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Args: desired_width (int, optional): width of the desired output ASCII Image. image_path (str, optional):path of the input image. If None then conversion will not work. \"\"\" if not image_path: raise ValueError self.ascii_str_map = [\" \", *(\"*$+?.%;:,@\")] self.ascii_image = \"\" self.image = Image.open(image_path).convert(\"L\") desired_height = desired_width * self.image.height / self.image.width self.image = self.image.resize((ceil(desired_width), ceil(desired_height)))","title":"__init__()"},{"location":"imageasascii/#pyrobotstxt.ImageAsASCII.map_to_ascii","text":"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. Source code in pyrobotstxt/__init__.py def map_to_ascii(self): \"\"\"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. \"\"\" str_container = \"\" # a container to hold ascii charcters for pixel in self.image.getdata(): str_container += self.ascii_str_map[pixel // 25] self.ascii_image = \"#\\t\" # Now transform the string container to column format. for i in range(0, len(str_container), self.image.width): self.ascii_image += ( \" \".join(str_container[i : i + self.image.width]) + \"\\n#\\t\" )","title":"map_to_ascii()"},{"location":"robotstxt/","text":"RobotsTxt Class \u00b6 Source code in pyrobotstxt/__init__.py class RobotsTxt: def __init__(self, version=\"\"): \"\"\"Intializes Robots.txt operations Args: version (str, optional): Version number (optional) for robots.txt. Defaults to \"\". \"\"\" self.user_agents = [] self.create_time = datetime.now() self.version = version self.image_branding = None self.header = \"\" # message added to the start of the output file. self.footer = \"\" # message added to the end of the output file. def read(self, robots_url): \"\"\"Read a Remote Robots.txt file from a given URL If robots_txt is missing a robots.txt file extention then it will be automatically added. Parsing will only be carried out if robots_url returns a valid response object. Args: robots_url (str): robots.txt url at a remote location. \"\"\" self.create_time = datetime.now() robots_url = get_corrected_url(robots_url, \"\") response = get_remote_content(robots_url) if response.status_code < 400: for ua_item in response.text.split(\"User-agent:\"): if ua_item: ua_content_items = [ ua_split_item.strip() for ua_split_item in ua_item.split(\"\\n\") if ua_split_item ] if not ua_content_items[0].startswith(\"#\"): ua = UserAgent(ua_name=ua_content_items[0]) ua.add_allow( [ it.split(\"Allow:\")[-1] for it in ua_content_items[1:] if it.startswith(\"Allow:\") ] ) ua.add_disallow( [ it.split(\"Disallow:\")[-1] for it in ua_content_items[1:] if it.startswith(\"Disallow:\") ] ) # TODO: Comments are not included Yet comment = [ it.split(\"# \")[-1] for it in ua_content_items[1:] if it.startswith(\"#\") ] self.add_user_agent(ua=ua) def write(self, file_path=\"robots.txt\"): \"\"\"write robots.txt file at a given file_path location. Args: file_path (str, optional): location of robots.txt file. Defaults to \"robots.txt\". \"\"\" with open(file_path, \"w\") as f: # include header if self.header: f.write(f\"# {self.header}\") # include user agents with consolidate text for ua in self.user_agents: ua.consolidate() f.write(ua.content) f.write(\"\\n\") # append ascii image, if available if self.image_branding: f.write(self.image_branding) # append footer message if self.footer: f.write(f\"\\n# {self.footer}\") def include_header(self, message=\"\", append_date=True): \"\"\"include header message with/without creation date. Args: message (str, optional): header or header message. Defaults to \"\". append_date (bool, optional): Append date/time to the header. Defaults to True. \"\"\" self.header = message if append_date: self.header += f\"\\n# Created on {self.create_time} using pyrobotstxt\" def include_footer(self, message=\"\"): \"\"\"include footer message Args: message (str, optional): footer message. Defaults to \"\". \"\"\" self.footer = message def include_image(self, image_path=None, desired_width=90): \"\"\"includes ascii image provided at image_file Args: image_path (str): location of image file. Defaults to None. desired_width (int, optional): desired width of ASCII image. Defaults to 90(chars). \"\"\" img = ImageAsASCII(image_path=image_path, desired_width=desired_width) img.map_to_ascii() self.image_branding = img.ascii_image def add_user_agent(self, ua): \"\"\"Add/Append user agent to RobotsTxt Args: ua (UserAgent): user agent to be included in final robots.txt file. \"\"\" self.user_agents.append(ua) def remove_user_agent(self, ua_name=\"\"): \"\"\"Remove user agent from RobotsTxt Args: ua_name (UserAgent): user agent to be removed from already included in robots.txt file. \"\"\" self.user_agents -= [ua for ua in self.user_agents if ua.name == ua_name] @staticmethod def robots_name(crawl_bot): \"\"\"Find robot name, if you know any keywrod about that crawl bot. Args: crawl_bot (str): description about the crawl bot. e.g. facebook Returns: (dict): all matching crawl bots with relevent information \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.capitalize() in ROBOTS[robot] } @staticmethod def robots_details(crawl_bot): \"\"\"Static Method to return details about any crawl bot. Args: crawl_bot (str): name of crawl bot Returns: (dict): information about all crawl bots matching to input string. \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.lower() == robot.lower() } __init__(version='') \u00b6 Intializes Robots.txt operations Parameters: Name Type Description Default version str Version number (optional) for robots.txt. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def __init__(self, version=\"\"): \"\"\"Intializes Robots.txt operations Args: version (str, optional): Version number (optional) for robots.txt. Defaults to \"\". \"\"\" self.user_agents = [] self.create_time = datetime.now() self.version = version self.image_branding = None self.header = \"\" # message added to the start of the output file. self.footer = \"\" # message added to the end of the output file. add_user_agent(ua) \u00b6 Add/Append user agent to RobotsTxt Parameters: Name Type Description Default ua UserAgent user agent to be included in final robots.txt file. required Source code in pyrobotstxt/__init__.py def add_user_agent(self, ua): \"\"\"Add/Append user agent to RobotsTxt Args: ua (UserAgent): user agent to be included in final robots.txt file. \"\"\" self.user_agents.append(ua) include_footer(message='') \u00b6 include footer message Parameters: Name Type Description Default message str footer message. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def include_footer(self, message=\"\"): \"\"\"include footer message Args: message (str, optional): footer message. Defaults to \"\". \"\"\" self.footer = message include_header(message='', append_date=True) \u00b6 include header message with/without creation date. Parameters: Name Type Description Default message str header or header message. Defaults to \u201c\u201d. '' append_date bool Append date/time to the header. Defaults to True. True Source code in pyrobotstxt/__init__.py def include_header(self, message=\"\", append_date=True): \"\"\"include header message with/without creation date. Args: message (str, optional): header or header message. Defaults to \"\". append_date (bool, optional): Append date/time to the header. Defaults to True. \"\"\" self.header = message if append_date: self.header += f\"\\n# Created on {self.create_time} using pyrobotstxt\" include_image(image_path=None, desired_width=90) \u00b6 includes ascii image provided at image_file Parameters: Name Type Description Default image_path str location of image file. Defaults to None. None desired_width int desired width of ASCII image. Defaults to 90(chars). 90 Source code in pyrobotstxt/__init__.py def include_image(self, image_path=None, desired_width=90): \"\"\"includes ascii image provided at image_file Args: image_path (str): location of image file. Defaults to None. desired_width (int, optional): desired width of ASCII image. Defaults to 90(chars). \"\"\" img = ImageAsASCII(image_path=image_path, desired_width=desired_width) img.map_to_ascii() self.image_branding = img.ascii_image read(robots_url) \u00b6 Read a Remote Robots.txt file from a given URL If robots_txt is missing a robots.txt file extention then it will be automatically added. Parsing will only be carried out if robots_url returns a valid response object. Parameters: Name Type Description Default robots_url str robots.txt url at a remote location. required Source code in pyrobotstxt/__init__.py def read(self, robots_url): \"\"\"Read a Remote Robots.txt file from a given URL If robots_txt is missing a robots.txt file extention then it will be automatically added. Parsing will only be carried out if robots_url returns a valid response object. Args: robots_url (str): robots.txt url at a remote location. \"\"\" self.create_time = datetime.now() robots_url = get_corrected_url(robots_url, \"\") response = get_remote_content(robots_url) if response.status_code < 400: for ua_item in response.text.split(\"User-agent:\"): if ua_item: ua_content_items = [ ua_split_item.strip() for ua_split_item in ua_item.split(\"\\n\") if ua_split_item ] if not ua_content_items[0].startswith(\"#\"): ua = UserAgent(ua_name=ua_content_items[0]) ua.add_allow( [ it.split(\"Allow:\")[-1] for it in ua_content_items[1:] if it.startswith(\"Allow:\") ] ) ua.add_disallow( [ it.split(\"Disallow:\")[-1] for it in ua_content_items[1:] if it.startswith(\"Disallow:\") ] ) # TODO: Comments are not included Yet comment = [ it.split(\"# \")[-1] for it in ua_content_items[1:] if it.startswith(\"#\") ] self.add_user_agent(ua=ua) remove_user_agent(ua_name='') \u00b6 Remove user agent from RobotsTxt Parameters: Name Type Description Default ua_name UserAgent user agent to be removed from already included in robots.txt file. '' Source code in pyrobotstxt/__init__.py def remove_user_agent(self, ua_name=\"\"): \"\"\"Remove user agent from RobotsTxt Args: ua_name (UserAgent): user agent to be removed from already included in robots.txt file. \"\"\" self.user_agents -= [ua for ua in self.user_agents if ua.name == ua_name] robots_details(crawl_bot) staticmethod \u00b6 Static Method to return details about any crawl bot. Parameters: Name Type Description Default crawl_bot str name of crawl bot required Returns: Type Description dict information about all crawl bots matching to input string. Source code in pyrobotstxt/__init__.py @staticmethod def robots_details(crawl_bot): \"\"\"Static Method to return details about any crawl bot. Args: crawl_bot (str): name of crawl bot Returns: (dict): information about all crawl bots matching to input string. \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.lower() == robot.lower() } robots_name(crawl_bot) staticmethod \u00b6 Find robot name, if you know any keywrod about that crawl bot. Parameters: Name Type Description Default crawl_bot str description about the crawl bot. e.g. facebook required Returns: Type Description dict all matching crawl bots with relevent information Source code in pyrobotstxt/__init__.py @staticmethod def robots_name(crawl_bot): \"\"\"Find robot name, if you know any keywrod about that crawl bot. Args: crawl_bot (str): description about the crawl bot. e.g. facebook Returns: (dict): all matching crawl bots with relevent information \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.capitalize() in ROBOTS[robot] } write(file_path='robots.txt') \u00b6 write robots.txt file at a given file_path location. Parameters: Name Type Description Default file_path str location of robots.txt file. Defaults to \u201crobots.txt\u201d. 'robots.txt' Source code in pyrobotstxt/__init__.py def write(self, file_path=\"robots.txt\"): \"\"\"write robots.txt file at a given file_path location. Args: file_path (str, optional): location of robots.txt file. Defaults to \"robots.txt\". \"\"\" with open(file_path, \"w\") as f: # include header if self.header: f.write(f\"# {self.header}\") # include user agents with consolidate text for ua in self.user_agents: ua.consolidate() f.write(ua.content) f.write(\"\\n\") # append ascii image, if available if self.image_branding: f.write(self.image_branding) # append footer message if self.footer: f.write(f\"\\n# {self.footer}\")","title":"RobotsTxt"},{"location":"robotstxt/#robotstxt-class","text":"Source code in pyrobotstxt/__init__.py class RobotsTxt: def __init__(self, version=\"\"): \"\"\"Intializes Robots.txt operations Args: version (str, optional): Version number (optional) for robots.txt. Defaults to \"\". \"\"\" self.user_agents = [] self.create_time = datetime.now() self.version = version self.image_branding = None self.header = \"\" # message added to the start of the output file. self.footer = \"\" # message added to the end of the output file. def read(self, robots_url): \"\"\"Read a Remote Robots.txt file from a given URL If robots_txt is missing a robots.txt file extention then it will be automatically added. Parsing will only be carried out if robots_url returns a valid response object. Args: robots_url (str): robots.txt url at a remote location. \"\"\" self.create_time = datetime.now() robots_url = get_corrected_url(robots_url, \"\") response = get_remote_content(robots_url) if response.status_code < 400: for ua_item in response.text.split(\"User-agent:\"): if ua_item: ua_content_items = [ ua_split_item.strip() for ua_split_item in ua_item.split(\"\\n\") if ua_split_item ] if not ua_content_items[0].startswith(\"#\"): ua = UserAgent(ua_name=ua_content_items[0]) ua.add_allow( [ it.split(\"Allow:\")[-1] for it in ua_content_items[1:] if it.startswith(\"Allow:\") ] ) ua.add_disallow( [ it.split(\"Disallow:\")[-1] for it in ua_content_items[1:] if it.startswith(\"Disallow:\") ] ) # TODO: Comments are not included Yet comment = [ it.split(\"# \")[-1] for it in ua_content_items[1:] if it.startswith(\"#\") ] self.add_user_agent(ua=ua) def write(self, file_path=\"robots.txt\"): \"\"\"write robots.txt file at a given file_path location. Args: file_path (str, optional): location of robots.txt file. Defaults to \"robots.txt\". \"\"\" with open(file_path, \"w\") as f: # include header if self.header: f.write(f\"# {self.header}\") # include user agents with consolidate text for ua in self.user_agents: ua.consolidate() f.write(ua.content) f.write(\"\\n\") # append ascii image, if available if self.image_branding: f.write(self.image_branding) # append footer message if self.footer: f.write(f\"\\n# {self.footer}\") def include_header(self, message=\"\", append_date=True): \"\"\"include header message with/without creation date. Args: message (str, optional): header or header message. Defaults to \"\". append_date (bool, optional): Append date/time to the header. Defaults to True. \"\"\" self.header = message if append_date: self.header += f\"\\n# Created on {self.create_time} using pyrobotstxt\" def include_footer(self, message=\"\"): \"\"\"include footer message Args: message (str, optional): footer message. Defaults to \"\". \"\"\" self.footer = message def include_image(self, image_path=None, desired_width=90): \"\"\"includes ascii image provided at image_file Args: image_path (str): location of image file. Defaults to None. desired_width (int, optional): desired width of ASCII image. Defaults to 90(chars). \"\"\" img = ImageAsASCII(image_path=image_path, desired_width=desired_width) img.map_to_ascii() self.image_branding = img.ascii_image def add_user_agent(self, ua): \"\"\"Add/Append user agent to RobotsTxt Args: ua (UserAgent): user agent to be included in final robots.txt file. \"\"\" self.user_agents.append(ua) def remove_user_agent(self, ua_name=\"\"): \"\"\"Remove user agent from RobotsTxt Args: ua_name (UserAgent): user agent to be removed from already included in robots.txt file. \"\"\" self.user_agents -= [ua for ua in self.user_agents if ua.name == ua_name] @staticmethod def robots_name(crawl_bot): \"\"\"Find robot name, if you know any keywrod about that crawl bot. Args: crawl_bot (str): description about the crawl bot. e.g. facebook Returns: (dict): all matching crawl bots with relevent information \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.capitalize() in ROBOTS[robot] } @staticmethod def robots_details(crawl_bot): \"\"\"Static Method to return details about any crawl bot. Args: crawl_bot (str): name of crawl bot Returns: (dict): information about all crawl bots matching to input string. \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.lower() == robot.lower() }","title":"RobotsTxt Class"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.__init__","text":"Intializes Robots.txt operations Parameters: Name Type Description Default version str Version number (optional) for robots.txt. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def __init__(self, version=\"\"): \"\"\"Intializes Robots.txt operations Args: version (str, optional): Version number (optional) for robots.txt. Defaults to \"\". \"\"\" self.user_agents = [] self.create_time = datetime.now() self.version = version self.image_branding = None self.header = \"\" # message added to the start of the output file. self.footer = \"\" # message added to the end of the output file.","title":"__init__()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.add_user_agent","text":"Add/Append user agent to RobotsTxt Parameters: Name Type Description Default ua UserAgent user agent to be included in final robots.txt file. required Source code in pyrobotstxt/__init__.py def add_user_agent(self, ua): \"\"\"Add/Append user agent to RobotsTxt Args: ua (UserAgent): user agent to be included in final robots.txt file. \"\"\" self.user_agents.append(ua)","title":"add_user_agent()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.include_footer","text":"include footer message Parameters: Name Type Description Default message str footer message. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def include_footer(self, message=\"\"): \"\"\"include footer message Args: message (str, optional): footer message. Defaults to \"\". \"\"\" self.footer = message","title":"include_footer()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.include_header","text":"include header message with/without creation date. Parameters: Name Type Description Default message str header or header message. Defaults to \u201c\u201d. '' append_date bool Append date/time to the header. Defaults to True. True Source code in pyrobotstxt/__init__.py def include_header(self, message=\"\", append_date=True): \"\"\"include header message with/without creation date. Args: message (str, optional): header or header message. Defaults to \"\". append_date (bool, optional): Append date/time to the header. Defaults to True. \"\"\" self.header = message if append_date: self.header += f\"\\n# Created on {self.create_time} using pyrobotstxt\"","title":"include_header()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.include_image","text":"includes ascii image provided at image_file Parameters: Name Type Description Default image_path str location of image file. Defaults to None. None desired_width int desired width of ASCII image. Defaults to 90(chars). 90 Source code in pyrobotstxt/__init__.py def include_image(self, image_path=None, desired_width=90): \"\"\"includes ascii image provided at image_file Args: image_path (str): location of image file. Defaults to None. desired_width (int, optional): desired width of ASCII image. Defaults to 90(chars). \"\"\" img = ImageAsASCII(image_path=image_path, desired_width=desired_width) img.map_to_ascii() self.image_branding = img.ascii_image","title":"include_image()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.read","text":"Read a Remote Robots.txt file from a given URL If robots_txt is missing a robots.txt file extention then it will be automatically added. Parsing will only be carried out if robots_url returns a valid response object. Parameters: Name Type Description Default robots_url str robots.txt url at a remote location. required Source code in pyrobotstxt/__init__.py def read(self, robots_url): \"\"\"Read a Remote Robots.txt file from a given URL If robots_txt is missing a robots.txt file extention then it will be automatically added. Parsing will only be carried out if robots_url returns a valid response object. Args: robots_url (str): robots.txt url at a remote location. \"\"\" self.create_time = datetime.now() robots_url = get_corrected_url(robots_url, \"\") response = get_remote_content(robots_url) if response.status_code < 400: for ua_item in response.text.split(\"User-agent:\"): if ua_item: ua_content_items = [ ua_split_item.strip() for ua_split_item in ua_item.split(\"\\n\") if ua_split_item ] if not ua_content_items[0].startswith(\"#\"): ua = UserAgent(ua_name=ua_content_items[0]) ua.add_allow( [ it.split(\"Allow:\")[-1] for it in ua_content_items[1:] if it.startswith(\"Allow:\") ] ) ua.add_disallow( [ it.split(\"Disallow:\")[-1] for it in ua_content_items[1:] if it.startswith(\"Disallow:\") ] ) # TODO: Comments are not included Yet comment = [ it.split(\"# \")[-1] for it in ua_content_items[1:] if it.startswith(\"#\") ] self.add_user_agent(ua=ua)","title":"read()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.remove_user_agent","text":"Remove user agent from RobotsTxt Parameters: Name Type Description Default ua_name UserAgent user agent to be removed from already included in robots.txt file. '' Source code in pyrobotstxt/__init__.py def remove_user_agent(self, ua_name=\"\"): \"\"\"Remove user agent from RobotsTxt Args: ua_name (UserAgent): user agent to be removed from already included in robots.txt file. \"\"\" self.user_agents -= [ua for ua in self.user_agents if ua.name == ua_name]","title":"remove_user_agent()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.robots_details","text":"Static Method to return details about any crawl bot. Parameters: Name Type Description Default crawl_bot str name of crawl bot required Returns: Type Description dict information about all crawl bots matching to input string. Source code in pyrobotstxt/__init__.py @staticmethod def robots_details(crawl_bot): \"\"\"Static Method to return details about any crawl bot. Args: crawl_bot (str): name of crawl bot Returns: (dict): information about all crawl bots matching to input string. \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.lower() == robot.lower() }","title":"robots_details()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.robots_name","text":"Find robot name, if you know any keywrod about that crawl bot. Parameters: Name Type Description Default crawl_bot str description about the crawl bot. e.g. facebook required Returns: Type Description dict all matching crawl bots with relevent information Source code in pyrobotstxt/__init__.py @staticmethod def robots_name(crawl_bot): \"\"\"Find robot name, if you know any keywrod about that crawl bot. Args: crawl_bot (str): description about the crawl bot. e.g. facebook Returns: (dict): all matching crawl bots with relevent information \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.capitalize() in ROBOTS[robot] }","title":"robots_name()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.write","text":"write robots.txt file at a given file_path location. Parameters: Name Type Description Default file_path str location of robots.txt file. Defaults to \u201crobots.txt\u201d. 'robots.txt' Source code in pyrobotstxt/__init__.py def write(self, file_path=\"robots.txt\"): \"\"\"write robots.txt file at a given file_path location. Args: file_path (str, optional): location of robots.txt file. Defaults to \"robots.txt\". \"\"\" with open(file_path, \"w\") as f: # include header if self.header: f.write(f\"# {self.header}\") # include user agents with consolidate text for ua in self.user_agents: ua.consolidate() f.write(ua.content) f.write(\"\\n\") # append ascii image, if available if self.image_branding: f.write(self.image_branding) # append footer message if self.footer: f.write(f\"\\n# {self.footer}\")","title":"write()"},{"location":"tutorial/","text":"Tutorial: How to Create Robots.txt File \u00b6 A user tutorial is available at serpwings pyrobotstxt tutorial page","title":"User Tutorial"},{"location":"tutorial/#tutorial-how-to-create-robotstxt-file","text":"A user tutorial is available at serpwings pyrobotstxt tutorial page","title":"Tutorial: How to Create Robots.txt File"},{"location":"useragent/","text":"UserAgent Class \u00b6 Source code in pyrobotstxt/__init__.py class UserAgent: def __init__(self, ua_name=\"*\", crawl_delay=0): \"\"\"Initialize UserAgent objet with a user-agent name and crawl delay varible. Args: ua_name (str, optional): name of the user-agent. Defaults to \"*\". crawl_delay (int, optional): crawl delay value for user agent/bots. Defaults to 0. \"\"\" self.user_agent_name = ua_name self.crawl_delay = crawl_delay self.sitemaps = [] # lists of sitemap for current UserAgent self.allowed = [] # lists of Allowed Items for current UserAgent self.disallowed = [] # lists of Disallowed Items for current UserAgent self.content = \"\" # consolidate content for robots.txt file def add_allow(self, allow_items, unique=True, comments=\"\"): \"\"\"Add allowed items/pages/slugs to current User Agent. Args: allow_items (str, list): single item or list of items allowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(allow_items, str): allow_items = [allow_items] if not isinstance(allow_items, list): print(\"not supported\", type(allow_items)) # raise exception raise TypeError else: self.allowed += allow_items if unique: self.allowed = list(set(self.allowed)) def remove_allow(self, allow_item): \"\"\"Remove any previously added allowed item from allowed list. Args: allow_item (str, list): item(s) to be removed. \"\"\" if allow_item in self.allowed: self.allowed -= [allow_item] def add_disallow(self, disallow_items, unique=True, comments=\"\"): \"\"\"Add disallowed items/pages/slugs to current User Agent. Args: disallow_items (str, list): single item or list of items disallowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(disallow_items, str): disallow_items = [disallow_items] if not isinstance(disallow_items, list): print(\"not supported\", type(disallow_items)) # raise exception raise TypeError else: self.disallowed += disallow_items if unique: self.disallowed = list(set(self.disallowed)) def remove_disallow(self, disallow_item): \"\"\"Remove any previously added disallowed item from allowed list. Args: disallow_item (str, list): item(s) to be removed. \"\"\" if disallow_item in self.disallowed: self.disallowed -= [disallow_item] def add_sitemap(self, site_map_path=None, comments=\"\"): \"\"\"add file path of sitemap to current user agent. Args: site_map_path (str): location of sitemap. Defaults to None. comments (str): any comments to include with sitemap path. Defaults to \"\". \"\"\" if not site_map_path: raise ValueError self.sitemaps.append(site_map_path) def remove_sitemap(self, site_map_path=None): \"\"\"remove a sitemap from current user agent. Args: site_map_path (str): sitemap file path to be removed. Defaults to None. \"\"\" if site_map_path in self.sitemaps: self.sitemaps -= [site_map_path] def disallow_pagination(self, prefix=\"/page/*\", comments=\"\"): \"\"\"Single function to disable pagination on a website using robots.txt file. Args: prefix (str, optional): prefix for pages (default - /page/). Defaults to \"/page/*\". comments (str, optional): human readable comments for inclusion. Defaults to \"\". \"\"\" self.add_disallow(disallow_item=prefix, comments=comments) def consolidate(self): \"\"\"consolidate all the information (allowed, disallowed, sitemaps) in single text string.\"\"\" self.content = f\"User-agent: {self.user_agent_name}\" # Support for including Crawl_delay. see feature request #1 if self.crawl_delay > 0: self.content += f\"\\nCrawl-delay: {self.crawl_delay}\\n\" if self.allowed: self.content += \"\\n# Allowed Patterns\\n\" self.content += \"\\n\".join([f\"Allow: {item}\" for item in self.allowed]) if self.disallowed: self.content += \"\\n\\n# Disallowed Patterns\\n\" self.content += \"\\n\".join([f\"Disallow: {item}\" for item in self.disallowed]) if self.sitemaps: self.content += \"\\n\\n# Site Maps\\n\" self.content += \"\\n\".join([f\"Sitemap: {item}\" for item in self.sitemaps]) self.content += \"\\n\\n\" __init__(ua_name='*', crawl_delay=0) \u00b6 Initialize UserAgent objet with a user-agent name and crawl delay varible. Parameters: Name Type Description Default ua_name str name of the user-agent. Defaults to \u201c*\u201d. '*' crawl_delay int crawl delay value for user agent/bots. Defaults to 0. 0 Source code in pyrobotstxt/__init__.py def __init__(self, ua_name=\"*\", crawl_delay=0): \"\"\"Initialize UserAgent objet with a user-agent name and crawl delay varible. Args: ua_name (str, optional): name of the user-agent. Defaults to \"*\". crawl_delay (int, optional): crawl delay value for user agent/bots. Defaults to 0. \"\"\" self.user_agent_name = ua_name self.crawl_delay = crawl_delay self.sitemaps = [] # lists of sitemap for current UserAgent self.allowed = [] # lists of Allowed Items for current UserAgent self.disallowed = [] # lists of Disallowed Items for current UserAgent self.content = \"\" # consolidate content for robots.txt file add_allow(allow_items, unique=True, comments='') \u00b6 Add allowed items/pages/slugs to current User Agent. Parameters: Name Type Description Default allow_items str , list single item or list of items allowed for current user agnet. required unique bool If True duplicate item stripped to single value. Defaults to True. True comments str Any comments for added value for human readability. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_allow(self, allow_items, unique=True, comments=\"\"): \"\"\"Add allowed items/pages/slugs to current User Agent. Args: allow_items (str, list): single item or list of items allowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(allow_items, str): allow_items = [allow_items] if not isinstance(allow_items, list): print(\"not supported\", type(allow_items)) # raise exception raise TypeError else: self.allowed += allow_items if unique: self.allowed = list(set(self.allowed)) add_disallow(disallow_items, unique=True, comments='') \u00b6 Add disallowed items/pages/slugs to current User Agent. Parameters: Name Type Description Default disallow_items str , list single item or list of items disallowed for current user agnet. required unique bool If True duplicate item stripped to single value. Defaults to True. True comments str Any comments for added value for human readability. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_disallow(self, disallow_items, unique=True, comments=\"\"): \"\"\"Add disallowed items/pages/slugs to current User Agent. Args: disallow_items (str, list): single item or list of items disallowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(disallow_items, str): disallow_items = [disallow_items] if not isinstance(disallow_items, list): print(\"not supported\", type(disallow_items)) # raise exception raise TypeError else: self.disallowed += disallow_items if unique: self.disallowed = list(set(self.disallowed)) add_sitemap(site_map_path=None, comments='') \u00b6 add file path of sitemap to current user agent. Parameters: Name Type Description Default site_map_path str location of sitemap. Defaults to None. None comments str any comments to include with sitemap path. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_sitemap(self, site_map_path=None, comments=\"\"): \"\"\"add file path of sitemap to current user agent. Args: site_map_path (str): location of sitemap. Defaults to None. comments (str): any comments to include with sitemap path. Defaults to \"\". \"\"\" if not site_map_path: raise ValueError self.sitemaps.append(site_map_path) consolidate() \u00b6 consolidate all the information (allowed, disallowed, sitemaps) in single text string. Source code in pyrobotstxt/__init__.py def consolidate(self): \"\"\"consolidate all the information (allowed, disallowed, sitemaps) in single text string.\"\"\" self.content = f\"User-agent: {self.user_agent_name}\" # Support for including Crawl_delay. see feature request #1 if self.crawl_delay > 0: self.content += f\"\\nCrawl-delay: {self.crawl_delay}\\n\" if self.allowed: self.content += \"\\n# Allowed Patterns\\n\" self.content += \"\\n\".join([f\"Allow: {item}\" for item in self.allowed]) if self.disallowed: self.content += \"\\n\\n# Disallowed Patterns\\n\" self.content += \"\\n\".join([f\"Disallow: {item}\" for item in self.disallowed]) if self.sitemaps: self.content += \"\\n\\n# Site Maps\\n\" self.content += \"\\n\".join([f\"Sitemap: {item}\" for item in self.sitemaps]) self.content += \"\\n\\n\" disallow_pagination(prefix='/page/*', comments='') \u00b6 Single function to disable pagination on a website using robots.txt file. Parameters: Name Type Description Default prefix str prefix for pages (default - /page/). Defaults to \u201c/page/*\u201d. '/page/*' comments str human readable comments for inclusion. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def disallow_pagination(self, prefix=\"/page/*\", comments=\"\"): \"\"\"Single function to disable pagination on a website using robots.txt file. Args: prefix (str, optional): prefix for pages (default - /page/). Defaults to \"/page/*\". comments (str, optional): human readable comments for inclusion. Defaults to \"\". \"\"\" self.add_disallow(disallow_item=prefix, comments=comments) remove_allow(allow_item) \u00b6 Remove any previously added allowed item from allowed list. Parameters: Name Type Description Default allow_item str , list item(s) to be removed. required Source code in pyrobotstxt/__init__.py def remove_allow(self, allow_item): \"\"\"Remove any previously added allowed item from allowed list. Args: allow_item (str, list): item(s) to be removed. \"\"\" if allow_item in self.allowed: self.allowed -= [allow_item] remove_disallow(disallow_item) \u00b6 Remove any previously added disallowed item from allowed list. Parameters: Name Type Description Default disallow_item str , list item(s) to be removed. required Source code in pyrobotstxt/__init__.py def remove_disallow(self, disallow_item): \"\"\"Remove any previously added disallowed item from allowed list. Args: disallow_item (str, list): item(s) to be removed. \"\"\" if disallow_item in self.disallowed: self.disallowed -= [disallow_item] remove_sitemap(site_map_path=None) \u00b6 remove a sitemap from current user agent. Parameters: Name Type Description Default site_map_path str sitemap file path to be removed. Defaults to None. None Source code in pyrobotstxt/__init__.py def remove_sitemap(self, site_map_path=None): \"\"\"remove a sitemap from current user agent. Args: site_map_path (str): sitemap file path to be removed. Defaults to None. \"\"\" if site_map_path in self.sitemaps: self.sitemaps -= [site_map_path]","title":"UserAgent"},{"location":"useragent/#useragent-class","text":"Source code in pyrobotstxt/__init__.py class UserAgent: def __init__(self, ua_name=\"*\", crawl_delay=0): \"\"\"Initialize UserAgent objet with a user-agent name and crawl delay varible. Args: ua_name (str, optional): name of the user-agent. Defaults to \"*\". crawl_delay (int, optional): crawl delay value for user agent/bots. Defaults to 0. \"\"\" self.user_agent_name = ua_name self.crawl_delay = crawl_delay self.sitemaps = [] # lists of sitemap for current UserAgent self.allowed = [] # lists of Allowed Items for current UserAgent self.disallowed = [] # lists of Disallowed Items for current UserAgent self.content = \"\" # consolidate content for robots.txt file def add_allow(self, allow_items, unique=True, comments=\"\"): \"\"\"Add allowed items/pages/slugs to current User Agent. Args: allow_items (str, list): single item or list of items allowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(allow_items, str): allow_items = [allow_items] if not isinstance(allow_items, list): print(\"not supported\", type(allow_items)) # raise exception raise TypeError else: self.allowed += allow_items if unique: self.allowed = list(set(self.allowed)) def remove_allow(self, allow_item): \"\"\"Remove any previously added allowed item from allowed list. Args: allow_item (str, list): item(s) to be removed. \"\"\" if allow_item in self.allowed: self.allowed -= [allow_item] def add_disallow(self, disallow_items, unique=True, comments=\"\"): \"\"\"Add disallowed items/pages/slugs to current User Agent. Args: disallow_items (str, list): single item or list of items disallowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(disallow_items, str): disallow_items = [disallow_items] if not isinstance(disallow_items, list): print(\"not supported\", type(disallow_items)) # raise exception raise TypeError else: self.disallowed += disallow_items if unique: self.disallowed = list(set(self.disallowed)) def remove_disallow(self, disallow_item): \"\"\"Remove any previously added disallowed item from allowed list. Args: disallow_item (str, list): item(s) to be removed. \"\"\" if disallow_item in self.disallowed: self.disallowed -= [disallow_item] def add_sitemap(self, site_map_path=None, comments=\"\"): \"\"\"add file path of sitemap to current user agent. Args: site_map_path (str): location of sitemap. Defaults to None. comments (str): any comments to include with sitemap path. Defaults to \"\". \"\"\" if not site_map_path: raise ValueError self.sitemaps.append(site_map_path) def remove_sitemap(self, site_map_path=None): \"\"\"remove a sitemap from current user agent. Args: site_map_path (str): sitemap file path to be removed. Defaults to None. \"\"\" if site_map_path in self.sitemaps: self.sitemaps -= [site_map_path] def disallow_pagination(self, prefix=\"/page/*\", comments=\"\"): \"\"\"Single function to disable pagination on a website using robots.txt file. Args: prefix (str, optional): prefix for pages (default - /page/). Defaults to \"/page/*\". comments (str, optional): human readable comments for inclusion. Defaults to \"\". \"\"\" self.add_disallow(disallow_item=prefix, comments=comments) def consolidate(self): \"\"\"consolidate all the information (allowed, disallowed, sitemaps) in single text string.\"\"\" self.content = f\"User-agent: {self.user_agent_name}\" # Support for including Crawl_delay. see feature request #1 if self.crawl_delay > 0: self.content += f\"\\nCrawl-delay: {self.crawl_delay}\\n\" if self.allowed: self.content += \"\\n# Allowed Patterns\\n\" self.content += \"\\n\".join([f\"Allow: {item}\" for item in self.allowed]) if self.disallowed: self.content += \"\\n\\n# Disallowed Patterns\\n\" self.content += \"\\n\".join([f\"Disallow: {item}\" for item in self.disallowed]) if self.sitemaps: self.content += \"\\n\\n# Site Maps\\n\" self.content += \"\\n\".join([f\"Sitemap: {item}\" for item in self.sitemaps]) self.content += \"\\n\\n\"","title":"UserAgent Class"},{"location":"useragent/#pyrobotstxt.UserAgent.__init__","text":"Initialize UserAgent objet with a user-agent name and crawl delay varible. Parameters: Name Type Description Default ua_name str name of the user-agent. Defaults to \u201c*\u201d. '*' crawl_delay int crawl delay value for user agent/bots. Defaults to 0. 0 Source code in pyrobotstxt/__init__.py def __init__(self, ua_name=\"*\", crawl_delay=0): \"\"\"Initialize UserAgent objet with a user-agent name and crawl delay varible. Args: ua_name (str, optional): name of the user-agent. Defaults to \"*\". crawl_delay (int, optional): crawl delay value for user agent/bots. Defaults to 0. \"\"\" self.user_agent_name = ua_name self.crawl_delay = crawl_delay self.sitemaps = [] # lists of sitemap for current UserAgent self.allowed = [] # lists of Allowed Items for current UserAgent self.disallowed = [] # lists of Disallowed Items for current UserAgent self.content = \"\" # consolidate content for robots.txt file","title":"__init__()"},{"location":"useragent/#pyrobotstxt.UserAgent.add_allow","text":"Add allowed items/pages/slugs to current User Agent. Parameters: Name Type Description Default allow_items str , list single item or list of items allowed for current user agnet. required unique bool If True duplicate item stripped to single value. Defaults to True. True comments str Any comments for added value for human readability. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_allow(self, allow_items, unique=True, comments=\"\"): \"\"\"Add allowed items/pages/slugs to current User Agent. Args: allow_items (str, list): single item or list of items allowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(allow_items, str): allow_items = [allow_items] if not isinstance(allow_items, list): print(\"not supported\", type(allow_items)) # raise exception raise TypeError else: self.allowed += allow_items if unique: self.allowed = list(set(self.allowed))","title":"add_allow()"},{"location":"useragent/#pyrobotstxt.UserAgent.add_disallow","text":"Add disallowed items/pages/slugs to current User Agent. Parameters: Name Type Description Default disallow_items str , list single item or list of items disallowed for current user agnet. required unique bool If True duplicate item stripped to single value. Defaults to True. True comments str Any comments for added value for human readability. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_disallow(self, disallow_items, unique=True, comments=\"\"): \"\"\"Add disallowed items/pages/slugs to current User Agent. Args: disallow_items (str, list): single item or list of items disallowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(disallow_items, str): disallow_items = [disallow_items] if not isinstance(disallow_items, list): print(\"not supported\", type(disallow_items)) # raise exception raise TypeError else: self.disallowed += disallow_items if unique: self.disallowed = list(set(self.disallowed))","title":"add_disallow()"},{"location":"useragent/#pyrobotstxt.UserAgent.add_sitemap","text":"add file path of sitemap to current user agent. Parameters: Name Type Description Default site_map_path str location of sitemap. Defaults to None. None comments str any comments to include with sitemap path. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_sitemap(self, site_map_path=None, comments=\"\"): \"\"\"add file path of sitemap to current user agent. Args: site_map_path (str): location of sitemap. Defaults to None. comments (str): any comments to include with sitemap path. Defaults to \"\". \"\"\" if not site_map_path: raise ValueError self.sitemaps.append(site_map_path)","title":"add_sitemap()"},{"location":"useragent/#pyrobotstxt.UserAgent.consolidate","text":"consolidate all the information (allowed, disallowed, sitemaps) in single text string. Source code in pyrobotstxt/__init__.py def consolidate(self): \"\"\"consolidate all the information (allowed, disallowed, sitemaps) in single text string.\"\"\" self.content = f\"User-agent: {self.user_agent_name}\" # Support for including Crawl_delay. see feature request #1 if self.crawl_delay > 0: self.content += f\"\\nCrawl-delay: {self.crawl_delay}\\n\" if self.allowed: self.content += \"\\n# Allowed Patterns\\n\" self.content += \"\\n\".join([f\"Allow: {item}\" for item in self.allowed]) if self.disallowed: self.content += \"\\n\\n# Disallowed Patterns\\n\" self.content += \"\\n\".join([f\"Disallow: {item}\" for item in self.disallowed]) if self.sitemaps: self.content += \"\\n\\n# Site Maps\\n\" self.content += \"\\n\".join([f\"Sitemap: {item}\" for item in self.sitemaps]) self.content += \"\\n\\n\"","title":"consolidate()"},{"location":"useragent/#pyrobotstxt.UserAgent.disallow_pagination","text":"Single function to disable pagination on a website using robots.txt file. Parameters: Name Type Description Default prefix str prefix for pages (default - /page/). Defaults to \u201c/page/*\u201d. '/page/*' comments str human readable comments for inclusion. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def disallow_pagination(self, prefix=\"/page/*\", comments=\"\"): \"\"\"Single function to disable pagination on a website using robots.txt file. Args: prefix (str, optional): prefix for pages (default - /page/). Defaults to \"/page/*\". comments (str, optional): human readable comments for inclusion. Defaults to \"\". \"\"\" self.add_disallow(disallow_item=prefix, comments=comments)","title":"disallow_pagination()"},{"location":"useragent/#pyrobotstxt.UserAgent.remove_allow","text":"Remove any previously added allowed item from allowed list. Parameters: Name Type Description Default allow_item str , list item(s) to be removed. required Source code in pyrobotstxt/__init__.py def remove_allow(self, allow_item): \"\"\"Remove any previously added allowed item from allowed list. Args: allow_item (str, list): item(s) to be removed. \"\"\" if allow_item in self.allowed: self.allowed -= [allow_item]","title":"remove_allow()"},{"location":"useragent/#pyrobotstxt.UserAgent.remove_disallow","text":"Remove any previously added disallowed item from allowed list. Parameters: Name Type Description Default disallow_item str , list item(s) to be removed. required Source code in pyrobotstxt/__init__.py def remove_disallow(self, disallow_item): \"\"\"Remove any previously added disallowed item from allowed list. Args: disallow_item (str, list): item(s) to be removed. \"\"\" if disallow_item in self.disallowed: self.disallowed -= [disallow_item]","title":"remove_disallow()"},{"location":"useragent/#pyrobotstxt.UserAgent.remove_sitemap","text":"remove a sitemap from current user agent. Parameters: Name Type Description Default site_map_path str sitemap file path to be removed. Defaults to None. None Source code in pyrobotstxt/__init__.py def remove_sitemap(self, site_map_path=None): \"\"\"remove a sitemap from current user agent. Args: site_map_path (str): sitemap file path to be removed. Defaults to None. \"\"\" if site_map_path in self.sitemaps: self.sitemaps -= [site_map_path]","title":"remove_sitemap()"}]}