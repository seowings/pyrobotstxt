{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"pyrobotstxt - A Python Package for robots.txt Files \u00b6 pyrobotstxt package can be used to (systematically) generate robots.txt files. Moreover, this package comes in handy for creating and including ASCII images in robots.txt files. In future releases, it would be possible to parse, analyze and manipulate robots.txt file generated using any software (not limited to pyrobotstxt ) Whats in pyrobotstxt? \u00b6 We believe in monolithic software development and created this tiny package that does its job without any bloat. It is useful for Createing robots.txt File Parsing a robots.txt File [in progress] Analyzing robots.txt File [in progress] How to Use pyrobotstxt? \u00b6 You can follow our basic user tutorial on how to use this library. If you are a developer or want to test latest version of pyrobotstxt, you might find information on our developers section useful. About Us \u00b6 seowings is an opensource project to write, develop and promote tools for Data Sciences and Digital Marketing.","title":"Home"},{"location":"#pyrobotstxt-a-python-package-for-robotstxt-files","text":"pyrobotstxt package can be used to (systematically) generate robots.txt files. Moreover, this package comes in handy for creating and including ASCII images in robots.txt files. In future releases, it would be possible to parse, analyze and manipulate robots.txt file generated using any software (not limited to pyrobotstxt )","title":"pyrobotstxt - A Python Package for robots.txt Files"},{"location":"#whats-in-pyrobotstxt","text":"We believe in monolithic software development and created this tiny package that does its job without any bloat. It is useful for Createing robots.txt File Parsing a robots.txt File [in progress] Analyzing robots.txt File [in progress]","title":"Whats in pyrobotstxt?"},{"location":"#how-to-use-pyrobotstxt","text":"You can follow our basic user tutorial on how to use this library. If you are a developer or want to test latest version of pyrobotstxt, you might find information on our developers section useful.","title":"How to Use pyrobotstxt?"},{"location":"#about-us","text":"seowings is an opensource project to write, develop and promote tools for Data Sciences and Digital Marketing.","title":"About Us"},{"location":"developers/","text":"Developers Tutorial \u00b6 How to Install pyrobotstxt on your Computer? \u00b6 You can install using the virtual environment. C:\\src\\pyrobotstxt> python -m venv .venv C:\\src\\pyrobotstxt> .\\.venv\\Scripts\\activate (.venv) C:\\src\\pyrobotstxt> pip install -e . How to Contribute to pyrobotstxt? \u00b6 Feature Suggestions \u00b6 Do you have any feature suggestions, improvements and then create an issue on this repository. Pull Requests \u00b6 Have you improved anything in pyrobotstxt, then please create a usuall pull request and we will merge it after review. Collaborations \u00b6 You can contact us using our website seowings.org/contact .","title":"Developer Tutorial"},{"location":"developers/#developers-tutorial","text":"","title":"Developers Tutorial"},{"location":"developers/#how-to-install-pyrobotstxt-on-your-computer","text":"You can install using the virtual environment. C:\\src\\pyrobotstxt> python -m venv .venv C:\\src\\pyrobotstxt> .\\.venv\\Scripts\\activate (.venv) C:\\src\\pyrobotstxt> pip install -e .","title":"How to Install pyrobotstxt on your Computer?"},{"location":"developers/#how-to-contribute-to-pyrobotstxt","text":"","title":"How to Contribute to pyrobotstxt?"},{"location":"developers/#feature-suggestions","text":"Do you have any feature suggestions, improvements and then create an issue on this repository.","title":"Feature Suggestions"},{"location":"developers/#pull-requests","text":"Have you improved anything in pyrobotstxt, then please create a usuall pull request and we will merge it after review.","title":"Pull Requests"},{"location":"developers/#collaborations","text":"You can contact us using our website seowings.org/contact .","title":"Collaborations"},{"location":"imageasascii/","text":"ImageAsASCII Class \u00b6 Class to Convert RGB/GRAYSCALE Images to ASCII (Text) format. Source code in pyrobotstxt/__init__.py class ImageAsASCII: \"\"\"Class to Convert RGB/GRAYSCALE Images to ASCII (Text) format.\"\"\" def __init__(self, image_path=None, desired_width=90): \"\"\"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Args: desired_width (int, optional): width of the desired output ASCII Image. image_path (str, optional):path of the input image. If None then conversion will not work. \"\"\" if not image_path: raise ValueError self.ascii_str_map = [\" \", *(\"*$+?.%;:,@\")] self.ascii_image = \"\" self.image = Image.open(image_path).convert(\"L\") desired_height = desired_width * self.image.height / self.image.width self.image = self.image.resize((ceil(desired_width), ceil(desired_height))) def map_to_ascii(self): \"\"\"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. \"\"\" str_container = \"\" # a container to hold ascii charcters for pixel in self.image.getdata(): str_container += self.ascii_str_map[pixel // 25] self.ascii_image = \"#\\t\" # Now transform the string container to column format. for i in range(0, len(str_container), self.image.width): self.ascii_image += ( \" \".join(str_container[i : i + self.image.width]) + \"\\n#\\t\" ) __init__(image_path=None, desired_width=90) \u00b6 intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Parameters: Name Type Description Default desired_width int width of the desired output ASCII Image. 90 image_path str path of the input image. If None then conversion will not work. None Source code in pyrobotstxt/__init__.py def __init__(self, image_path=None, desired_width=90): \"\"\"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Args: desired_width (int, optional): width of the desired output ASCII Image. image_path (str, optional):path of the input image. If None then conversion will not work. \"\"\" if not image_path: raise ValueError self.ascii_str_map = [\" \", *(\"*$+?.%;:,@\")] self.ascii_image = \"\" self.image = Image.open(image_path).convert(\"L\") desired_height = desired_width * self.image.height / self.image.width self.image = self.image.resize((ceil(desired_width), ceil(desired_height))) map_to_ascii() \u00b6 map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. Source code in pyrobotstxt/__init__.py def map_to_ascii(self): \"\"\"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. \"\"\" str_container = \"\" # a container to hold ascii charcters for pixel in self.image.getdata(): str_container += self.ascii_str_map[pixel // 25] self.ascii_image = \"#\\t\" # Now transform the string container to column format. for i in range(0, len(str_container), self.image.width): self.ascii_image += ( \" \".join(str_container[i : i + self.image.width]) + \"\\n#\\t\" )","title":"ImageAsASCII"},{"location":"imageasascii/#imageasascii-class","text":"Class to Convert RGB/GRAYSCALE Images to ASCII (Text) format. Source code in pyrobotstxt/__init__.py class ImageAsASCII: \"\"\"Class to Convert RGB/GRAYSCALE Images to ASCII (Text) format.\"\"\" def __init__(self, image_path=None, desired_width=90): \"\"\"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Args: desired_width (int, optional): width of the desired output ASCII Image. image_path (str, optional):path of the input image. If None then conversion will not work. \"\"\" if not image_path: raise ValueError self.ascii_str_map = [\" \", *(\"*$+?.%;:,@\")] self.ascii_image = \"\" self.image = Image.open(image_path).convert(\"L\") desired_height = desired_width * self.image.height / self.image.width self.image = self.image.resize((ceil(desired_width), ceil(desired_height))) def map_to_ascii(self): \"\"\"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. \"\"\" str_container = \"\" # a container to hold ascii charcters for pixel in self.image.getdata(): str_container += self.ascii_str_map[pixel // 25] self.ascii_image = \"#\\t\" # Now transform the string container to column format. for i in range(0, len(str_container), self.image.width): self.ascii_image += ( \" \".join(str_container[i : i + self.image.width]) + \"\\n#\\t\" )","title":"ImageAsASCII Class"},{"location":"imageasascii/#pyrobotstxt.ImageAsASCII.__init__","text":"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Parameters: Name Type Description Default desired_width int width of the desired output ASCII Image. 90 image_path str path of the input image. If None then conversion will not work. None Source code in pyrobotstxt/__init__.py def __init__(self, image_path=None, desired_width=90): \"\"\"intializes an object of ImageAsASCII class. A user need to specify desired (ascii) image width and the path of RGB/Gray Image. Args: desired_width (int, optional): width of the desired output ASCII Image. image_path (str, optional):path of the input image. If None then conversion will not work. \"\"\" if not image_path: raise ValueError self.ascii_str_map = [\" \", *(\"*$+?.%;:,@\")] self.ascii_image = \"\" self.image = Image.open(image_path).convert(\"L\") desired_height = desired_width * self.image.height / self.image.width self.image = self.image.resize((ceil(desired_width), ceil(desired_height)))","title":"__init__()"},{"location":"imageasascii/#pyrobotstxt.ImageAsASCII.map_to_ascii","text":"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. Source code in pyrobotstxt/__init__.py def map_to_ascii(self): \"\"\"map each pixel of indvidual image to a respective ascii value from ascii_str_map. This is achieved by deviding each pixel to ca. 10 equal parts (//25) and then maped to respecive value. \"\"\" str_container = \"\" # a container to hold ascii charcters for pixel in self.image.getdata(): str_container += self.ascii_str_map[pixel // 25] self.ascii_image = \"#\\t\" # Now transform the string container to column format. for i in range(0, len(str_container), self.image.width): self.ascii_image += ( \" \".join(str_container[i : i + self.image.width]) + \"\\n#\\t\" )","title":"map_to_ascii()"},{"location":"robotstxt/","text":"RobotsTxt Class \u00b6 Source code in pyrobotstxt/__init__.py class RobotsTxt: def __init__(self, version=\"\"): \"\"\"Intializes Robots.txt operations Args: version (str, optional): Version number (optional) for robots.txt. Defaults to \"\". \"\"\" self.user_agents = [] self.create_time = datetime.now() self.version = version self.image_branding = None self.header = \"\" # message added to the start of the output file. self.footer = \"\" # message added to the end of the output file. def read(self): \"\"\"read a robots.txt File (TODO)\"\"\" self.create_time = datetime.now() def write(self, file_path=\"robots.txt\"): \"\"\"write robots.txt file at a given file_path location. Args: file_path (str, optional): location of robots.txt file. Defaults to \"robots.txt\". \"\"\" with open(file_path, \"w\") as f: # include header if self.header: f.write(f\"# {self.header}\") # include user agents with consolidate text for ua in self.user_agents: ua.consolidate() f.write(ua.content) f.write(\"\\n\\n\") # append ascii image, if available if self.image_branding: f.write(self.image_branding) # append footer message if self.footer: f.write(f\"\\n\\n# {self.footer}\") def include_header(self, message=\"\", append_date=True): \"\"\"include header message with/without creation date. Args: message (str, optional): header or header message. Defaults to \"\". append_date (bool, optional): Append date/time to the header. Defaults to True. \"\"\" self.header = f\"{message}\" if append_date: self.header += f\"\\n# Created on {self.create_time} using pyrobotstxt\" def include_footer(self, message=\"\"): \"\"\"include footer message Args: message (str, optional): footer message. Defaults to \"\". \"\"\" self.footer = message def include_image(self, image_path=None, desired_width=90): \"\"\"includes ascii image provided at image_file Args: image_path (str): location of image file. Defaults to None. desired_width (int, optional): desired width of ASCII image. Defaults to 90(chars). \"\"\" img = ImageAsASCII(image_path=image_path, desired_width=desired_width) img.map_to_ascii() self.image_branding = img.ascii_image def add_user_agent(self, ua): \"\"\"Add/Append user agent to RobotsTxt Args: ua (UserAgent): user agent to be included in final robots.txt file. \"\"\" self.user_agents.append(ua) def remove_user_agent(self, ua_name=\"\"): \"\"\"Remove user agent from RobotsTxt Args: ua_name (UserAgent): user agent to be removed from already included in robots.txt file. \"\"\" self.user_agents -= [ua for ua in self.user_agents if ua.name == ua_name] @staticmethod def robots_name(crawl_bot): \"\"\"Find robot name, if you know any keywrod about that crawl bot. Args: crawl_bot (str): description about the crawl bot. e.g. facebook Returns: (dict): all matching crawl bots with relevent information \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.capitalize() in ROBOTS[robot] } @staticmethod def robots_details(crawl_bot): \"\"\"Static Method to return details about any crawl bot. Args: crawl_bot (str): name of crawl bot Returns: (dict): information about all crawl bots matching to input string. \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.lower() == robot.lower() } __init__(version='') \u00b6 Intializes Robots.txt operations Parameters: Name Type Description Default version str Version number (optional) for robots.txt. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def __init__(self, version=\"\"): \"\"\"Intializes Robots.txt operations Args: version (str, optional): Version number (optional) for robots.txt. Defaults to \"\". \"\"\" self.user_agents = [] self.create_time = datetime.now() self.version = version self.image_branding = None self.header = \"\" # message added to the start of the output file. self.footer = \"\" # message added to the end of the output file. add_user_agent(ua) \u00b6 Add/Append user agent to RobotsTxt Parameters: Name Type Description Default ua UserAgent user agent to be included in final robots.txt file. required Source code in pyrobotstxt/__init__.py def add_user_agent(self, ua): \"\"\"Add/Append user agent to RobotsTxt Args: ua (UserAgent): user agent to be included in final robots.txt file. \"\"\" self.user_agents.append(ua) include_footer(message='') \u00b6 include footer message Parameters: Name Type Description Default message str footer message. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def include_footer(self, message=\"\"): \"\"\"include footer message Args: message (str, optional): footer message. Defaults to \"\". \"\"\" self.footer = message include_header(message='', append_date=True) \u00b6 include header message with/without creation date. Parameters: Name Type Description Default message str header or header message. Defaults to \u201c\u201d. '' append_date bool Append date/time to the header. Defaults to True. True Source code in pyrobotstxt/__init__.py def include_header(self, message=\"\", append_date=True): \"\"\"include header message with/without creation date. Args: message (str, optional): header or header message. Defaults to \"\". append_date (bool, optional): Append date/time to the header. Defaults to True. \"\"\" self.header = f\"{message}\" if append_date: self.header += f\"\\n# Created on {self.create_time} using pyrobotstxt\" include_image(image_path=None, desired_width=90) \u00b6 includes ascii image provided at image_file Parameters: Name Type Description Default image_path str location of image file. Defaults to None. None desired_width int desired width of ASCII image. Defaults to 90(chars). 90 Source code in pyrobotstxt/__init__.py def include_image(self, image_path=None, desired_width=90): \"\"\"includes ascii image provided at image_file Args: image_path (str): location of image file. Defaults to None. desired_width (int, optional): desired width of ASCII image. Defaults to 90(chars). \"\"\" img = ImageAsASCII(image_path=image_path, desired_width=desired_width) img.map_to_ascii() self.image_branding = img.ascii_image read() \u00b6 read a robots.txt File (TODO) Source code in pyrobotstxt/__init__.py def read(self): \"\"\"read a robots.txt File (TODO)\"\"\" self.create_time = datetime.now() remove_user_agent(ua_name='') \u00b6 Remove user agent from RobotsTxt Parameters: Name Type Description Default ua_name UserAgent user agent to be removed from already included in robots.txt file. '' Source code in pyrobotstxt/__init__.py def remove_user_agent(self, ua_name=\"\"): \"\"\"Remove user agent from RobotsTxt Args: ua_name (UserAgent): user agent to be removed from already included in robots.txt file. \"\"\" self.user_agents -= [ua for ua in self.user_agents if ua.name == ua_name] robots_details(crawl_bot) staticmethod \u00b6 Static Method to return details about any crawl bot. Parameters: Name Type Description Default crawl_bot str name of crawl bot required Returns: Type Description dict information about all crawl bots matching to input string. Source code in pyrobotstxt/__init__.py @staticmethod def robots_details(crawl_bot): \"\"\"Static Method to return details about any crawl bot. Args: crawl_bot (str): name of crawl bot Returns: (dict): information about all crawl bots matching to input string. \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.lower() == robot.lower() } robots_name(crawl_bot) staticmethod \u00b6 Find robot name, if you know any keywrod about that crawl bot. Parameters: Name Type Description Default crawl_bot str description about the crawl bot. e.g. facebook required Returns: Type Description dict all matching crawl bots with relevent information Source code in pyrobotstxt/__init__.py @staticmethod def robots_name(crawl_bot): \"\"\"Find robot name, if you know any keywrod about that crawl bot. Args: crawl_bot (str): description about the crawl bot. e.g. facebook Returns: (dict): all matching crawl bots with relevent information \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.capitalize() in ROBOTS[robot] } write(file_path='robots.txt') \u00b6 write robots.txt file at a given file_path location. Parameters: Name Type Description Default file_path str location of robots.txt file. Defaults to \u201crobots.txt\u201d. 'robots.txt' Source code in pyrobotstxt/__init__.py def write(self, file_path=\"robots.txt\"): \"\"\"write robots.txt file at a given file_path location. Args: file_path (str, optional): location of robots.txt file. Defaults to \"robots.txt\". \"\"\" with open(file_path, \"w\") as f: # include header if self.header: f.write(f\"# {self.header}\") # include user agents with consolidate text for ua in self.user_agents: ua.consolidate() f.write(ua.content) f.write(\"\\n\\n\") # append ascii image, if available if self.image_branding: f.write(self.image_branding) # append footer message if self.footer: f.write(f\"\\n\\n# {self.footer}\")","title":"RobotsTxt"},{"location":"robotstxt/#robotstxt-class","text":"Source code in pyrobotstxt/__init__.py class RobotsTxt: def __init__(self, version=\"\"): \"\"\"Intializes Robots.txt operations Args: version (str, optional): Version number (optional) for robots.txt. Defaults to \"\". \"\"\" self.user_agents = [] self.create_time = datetime.now() self.version = version self.image_branding = None self.header = \"\" # message added to the start of the output file. self.footer = \"\" # message added to the end of the output file. def read(self): \"\"\"read a robots.txt File (TODO)\"\"\" self.create_time = datetime.now() def write(self, file_path=\"robots.txt\"): \"\"\"write robots.txt file at a given file_path location. Args: file_path (str, optional): location of robots.txt file. Defaults to \"robots.txt\". \"\"\" with open(file_path, \"w\") as f: # include header if self.header: f.write(f\"# {self.header}\") # include user agents with consolidate text for ua in self.user_agents: ua.consolidate() f.write(ua.content) f.write(\"\\n\\n\") # append ascii image, if available if self.image_branding: f.write(self.image_branding) # append footer message if self.footer: f.write(f\"\\n\\n# {self.footer}\") def include_header(self, message=\"\", append_date=True): \"\"\"include header message with/without creation date. Args: message (str, optional): header or header message. Defaults to \"\". append_date (bool, optional): Append date/time to the header. Defaults to True. \"\"\" self.header = f\"{message}\" if append_date: self.header += f\"\\n# Created on {self.create_time} using pyrobotstxt\" def include_footer(self, message=\"\"): \"\"\"include footer message Args: message (str, optional): footer message. Defaults to \"\". \"\"\" self.footer = message def include_image(self, image_path=None, desired_width=90): \"\"\"includes ascii image provided at image_file Args: image_path (str): location of image file. Defaults to None. desired_width (int, optional): desired width of ASCII image. Defaults to 90(chars). \"\"\" img = ImageAsASCII(image_path=image_path, desired_width=desired_width) img.map_to_ascii() self.image_branding = img.ascii_image def add_user_agent(self, ua): \"\"\"Add/Append user agent to RobotsTxt Args: ua (UserAgent): user agent to be included in final robots.txt file. \"\"\" self.user_agents.append(ua) def remove_user_agent(self, ua_name=\"\"): \"\"\"Remove user agent from RobotsTxt Args: ua_name (UserAgent): user agent to be removed from already included in robots.txt file. \"\"\" self.user_agents -= [ua for ua in self.user_agents if ua.name == ua_name] @staticmethod def robots_name(crawl_bot): \"\"\"Find robot name, if you know any keywrod about that crawl bot. Args: crawl_bot (str): description about the crawl bot. e.g. facebook Returns: (dict): all matching crawl bots with relevent information \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.capitalize() in ROBOTS[robot] } @staticmethod def robots_details(crawl_bot): \"\"\"Static Method to return details about any crawl bot. Args: crawl_bot (str): name of crawl bot Returns: (dict): information about all crawl bots matching to input string. \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.lower() == robot.lower() }","title":"RobotsTxt Class"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.__init__","text":"Intializes Robots.txt operations Parameters: Name Type Description Default version str Version number (optional) for robots.txt. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def __init__(self, version=\"\"): \"\"\"Intializes Robots.txt operations Args: version (str, optional): Version number (optional) for robots.txt. Defaults to \"\". \"\"\" self.user_agents = [] self.create_time = datetime.now() self.version = version self.image_branding = None self.header = \"\" # message added to the start of the output file. self.footer = \"\" # message added to the end of the output file.","title":"__init__()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.add_user_agent","text":"Add/Append user agent to RobotsTxt Parameters: Name Type Description Default ua UserAgent user agent to be included in final robots.txt file. required Source code in pyrobotstxt/__init__.py def add_user_agent(self, ua): \"\"\"Add/Append user agent to RobotsTxt Args: ua (UserAgent): user agent to be included in final robots.txt file. \"\"\" self.user_agents.append(ua)","title":"add_user_agent()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.include_footer","text":"include footer message Parameters: Name Type Description Default message str footer message. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def include_footer(self, message=\"\"): \"\"\"include footer message Args: message (str, optional): footer message. Defaults to \"\". \"\"\" self.footer = message","title":"include_footer()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.include_header","text":"include header message with/without creation date. Parameters: Name Type Description Default message str header or header message. Defaults to \u201c\u201d. '' append_date bool Append date/time to the header. Defaults to True. True Source code in pyrobotstxt/__init__.py def include_header(self, message=\"\", append_date=True): \"\"\"include header message with/without creation date. Args: message (str, optional): header or header message. Defaults to \"\". append_date (bool, optional): Append date/time to the header. Defaults to True. \"\"\" self.header = f\"{message}\" if append_date: self.header += f\"\\n# Created on {self.create_time} using pyrobotstxt\"","title":"include_header()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.include_image","text":"includes ascii image provided at image_file Parameters: Name Type Description Default image_path str location of image file. Defaults to None. None desired_width int desired width of ASCII image. Defaults to 90(chars). 90 Source code in pyrobotstxt/__init__.py def include_image(self, image_path=None, desired_width=90): \"\"\"includes ascii image provided at image_file Args: image_path (str): location of image file. Defaults to None. desired_width (int, optional): desired width of ASCII image. Defaults to 90(chars). \"\"\" img = ImageAsASCII(image_path=image_path, desired_width=desired_width) img.map_to_ascii() self.image_branding = img.ascii_image","title":"include_image()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.read","text":"read a robots.txt File (TODO) Source code in pyrobotstxt/__init__.py def read(self): \"\"\"read a robots.txt File (TODO)\"\"\" self.create_time = datetime.now()","title":"read()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.remove_user_agent","text":"Remove user agent from RobotsTxt Parameters: Name Type Description Default ua_name UserAgent user agent to be removed from already included in robots.txt file. '' Source code in pyrobotstxt/__init__.py def remove_user_agent(self, ua_name=\"\"): \"\"\"Remove user agent from RobotsTxt Args: ua_name (UserAgent): user agent to be removed from already included in robots.txt file. \"\"\" self.user_agents -= [ua for ua in self.user_agents if ua.name == ua_name]","title":"remove_user_agent()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.robots_details","text":"Static Method to return details about any crawl bot. Parameters: Name Type Description Default crawl_bot str name of crawl bot required Returns: Type Description dict information about all crawl bots matching to input string. Source code in pyrobotstxt/__init__.py @staticmethod def robots_details(crawl_bot): \"\"\"Static Method to return details about any crawl bot. Args: crawl_bot (str): name of crawl bot Returns: (dict): information about all crawl bots matching to input string. \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.lower() == robot.lower() }","title":"robots_details()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.robots_name","text":"Find robot name, if you know any keywrod about that crawl bot. Parameters: Name Type Description Default crawl_bot str description about the crawl bot. e.g. facebook required Returns: Type Description dict all matching crawl bots with relevent information Source code in pyrobotstxt/__init__.py @staticmethod def robots_name(crawl_bot): \"\"\"Find robot name, if you know any keywrod about that crawl bot. Args: crawl_bot (str): description about the crawl bot. e.g. facebook Returns: (dict): all matching crawl bots with relevent information \"\"\" return { robot: ROBOTS[robot] for robot in ROBOTS if crawl_bot.capitalize() in ROBOTS[robot] }","title":"robots_name()"},{"location":"robotstxt/#pyrobotstxt.RobotsTxt.write","text":"write robots.txt file at a given file_path location. Parameters: Name Type Description Default file_path str location of robots.txt file. Defaults to \u201crobots.txt\u201d. 'robots.txt' Source code in pyrobotstxt/__init__.py def write(self, file_path=\"robots.txt\"): \"\"\"write robots.txt file at a given file_path location. Args: file_path (str, optional): location of robots.txt file. Defaults to \"robots.txt\". \"\"\" with open(file_path, \"w\") as f: # include header if self.header: f.write(f\"# {self.header}\") # include user agents with consolidate text for ua in self.user_agents: ua.consolidate() f.write(ua.content) f.write(\"\\n\\n\") # append ascii image, if available if self.image_branding: f.write(self.image_branding) # append footer message if self.footer: f.write(f\"\\n\\n# {self.footer}\")","title":"write()"},{"location":"tutorial/","text":"Tutorial: How to Create Robots.txt File \u00b6 In this tutorial, I will show how you can use pyrobotstxt to create Robots.txt files. Installation \u00b6 You need to install pyrobotstxt package using pip with the following command. It should also work with other methods, e.g. pipenv or poetry . If you encounter any installation problems, please create an issue . pip install pyrobotstxt Usage \u00b6 There are several use cases for pyrobotstxt . Basic \u00b6 You can use it to check the details of a search bot. Just specify a keyword, e.g. Google , and it will provide a list of Google bots in the pyrobotstxt database. from pyrobotstxt import RobotsTxt print(RobotsTxt().robots_name(\"Google\")) You can also do a reverse search by providing a robot name. Again, you will get details about that bot. from pyrobotstxt import RobotsTxt print(RobotsTxt().robots_details(\"Googlebot\")) Advance \u00b6 You can create a robot file by creating an object of the RobotsTxt class. from pyrobotstxt import RobotsTxt robots_file = RobotsTxt() You can add a header and footer section. These are comments for humans looking into this robots.txt file. In the header section, you can also include file creation date . It is handy for archiving purposes. robots_file.include_header(\"Welcome Crawlers\", append_date=True) robots_file.include_footer(\"Good Bye Crawlers\") In robots.txt file, rules are specified as per user agent. pyrobotstxt offer a UserAgent class. You can use it to create multiple user agent. Default user agent is * . ua_general = UserAgent(name=\"*\") After creating a user agent, you can add all those routes/pages/images you want to Allow or Disallow. ua_general.add_allow( allow_items=[\"/home\", \"/deep\", \"/home\"], unique=True, comments=\"This is a list of allowed items\", ) ua_general.add_disallow( disallow_items=[\"/nopi$\", \"/topi?a\", \"/img*.png$\"], unique=True, comments=\"This is a list of allowed items\", ) Here is a complete example of a user agent. You can also include a sitemap of your website. ua_general_google = UserAgent(name=\"Google\") ua_general_google.add_allow( allow_items=[\"/home\", \"/deep\", \"/home\"], unique=True, comments=\"This is a list of allowed items\", ) ua_general_google.add_disallow( disallow_items=[\"/nopi$\", \"/topi?a\", \"/img*.png$\"], unique=True, comments=\"This is a list of allowed items\", ) ua_general_google.add_sitemap(\"https://seowings.org/sitemap.xml\") After you have prepared user agents, you can add them to the RobotsTxt object. This object keeps a list of all the user agents. robots_file.add_user_agent(ua_general) robots_file.add_user_agent(ua_general_google) You can also include any image ( ASCII format) in your robots.txt file. For example, add the following command in your script/program to include an ASCII image in your robots.txt file. robots_file.include_image(\"logo_dark.png\", 90) In the end, you can save this file to the desired location. The default name of the file is robots.txt . robots_file.write(\"robots.txt\")","title":"User Tutorial"},{"location":"tutorial/#tutorial-how-to-create-robotstxt-file","text":"In this tutorial, I will show how you can use pyrobotstxt to create Robots.txt files.","title":"Tutorial: How to Create Robots.txt File"},{"location":"tutorial/#installation","text":"You need to install pyrobotstxt package using pip with the following command. It should also work with other methods, e.g. pipenv or poetry . If you encounter any installation problems, please create an issue . pip install pyrobotstxt","title":"Installation"},{"location":"tutorial/#usage","text":"There are several use cases for pyrobotstxt .","title":"Usage"},{"location":"tutorial/#basic","text":"You can use it to check the details of a search bot. Just specify a keyword, e.g. Google , and it will provide a list of Google bots in the pyrobotstxt database. from pyrobotstxt import RobotsTxt print(RobotsTxt().robots_name(\"Google\")) You can also do a reverse search by providing a robot name. Again, you will get details about that bot. from pyrobotstxt import RobotsTxt print(RobotsTxt().robots_details(\"Googlebot\"))","title":"Basic"},{"location":"tutorial/#advance","text":"You can create a robot file by creating an object of the RobotsTxt class. from pyrobotstxt import RobotsTxt robots_file = RobotsTxt() You can add a header and footer section. These are comments for humans looking into this robots.txt file. In the header section, you can also include file creation date . It is handy for archiving purposes. robots_file.include_header(\"Welcome Crawlers\", append_date=True) robots_file.include_footer(\"Good Bye Crawlers\") In robots.txt file, rules are specified as per user agent. pyrobotstxt offer a UserAgent class. You can use it to create multiple user agent. Default user agent is * . ua_general = UserAgent(name=\"*\") After creating a user agent, you can add all those routes/pages/images you want to Allow or Disallow. ua_general.add_allow( allow_items=[\"/home\", \"/deep\", \"/home\"], unique=True, comments=\"This is a list of allowed items\", ) ua_general.add_disallow( disallow_items=[\"/nopi$\", \"/topi?a\", \"/img*.png$\"], unique=True, comments=\"This is a list of allowed items\", ) Here is a complete example of a user agent. You can also include a sitemap of your website. ua_general_google = UserAgent(name=\"Google\") ua_general_google.add_allow( allow_items=[\"/home\", \"/deep\", \"/home\"], unique=True, comments=\"This is a list of allowed items\", ) ua_general_google.add_disallow( disallow_items=[\"/nopi$\", \"/topi?a\", \"/img*.png$\"], unique=True, comments=\"This is a list of allowed items\", ) ua_general_google.add_sitemap(\"https://seowings.org/sitemap.xml\") After you have prepared user agents, you can add them to the RobotsTxt object. This object keeps a list of all the user agents. robots_file.add_user_agent(ua_general) robots_file.add_user_agent(ua_general_google) You can also include any image ( ASCII format) in your robots.txt file. For example, add the following command in your script/program to include an ASCII image in your robots.txt file. robots_file.include_image(\"logo_dark.png\", 90) In the end, you can save this file to the desired location. The default name of the file is robots.txt . robots_file.write(\"robots.txt\")","title":"Advance"},{"location":"useragent/","text":"UserAgent Class \u00b6 Source code in pyrobotstxt/__init__.py class UserAgent: def __init__(self, ua_name=\"*\", crawl_delay=0): \"\"\"Initialize UserAgent objet with a user-agent name and crawl delay varible. Args: ua_name (str, optional): name of the user-agent. Defaults to \"*\". crawl_delay (int, optional): crawl delay value for user agent/bots. Defaults to 0. \"\"\" self.user_agent_name = ua_name self.crawl_delay = crawl_delay self.sitemaps = [] # lists of sitemap for current UserAgent self.allowed = [] # lists of Allowed Items for current UserAgent self.disallowed = [] # lists of Disallowed Items for current UserAgent self.content = \"\" # consolidate content for robots.txt file def add_allow(self, allow_items, unique=True, comments=\"\"): \"\"\"Add allowed items/pages/slugs to current User Agent. Args: allow_items (str, list): single item or list of items allowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(allow_items, str): allow_items = [allow_items] if not isinstance(allow_items, list): print(\"not supported\", type(allow_items)) # raise exception raise TypeError else: self.allowed += allow_items if unique: self.allowed = list(set(self.allowed)) def remove_allow(self, allow_item): \"\"\"Remove any previously added allowed item from allowed list. Args: allow_item (str, list): item(s) to be removed. \"\"\" if allow_item in self.allowed: self.allowed -= [allow_item] def add_disallow(self, disallow_items, unique=True, comments=\"\"): \"\"\"Add disallowed items/pages/slugs to current User Agent. Args: disallow_items (str, list): single item or list of items disallowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(disallow_items, str): disallow_items = [disallow_items] if not isinstance(disallow_items, list): print(\"not supported\", type(disallow_items)) # raise exception raise TypeError else: self.disallowed += disallow_items if unique: self.disallowed = list(set(self.disallowed)) def remove_disallow(self, disallow_item): \"\"\"Remove any previously added disallowed item from allowed list. Args: disallow_item (str, list): item(s) to be removed. \"\"\" if disallow_item in self.disallowed: self.disallowed -= [disallow_item] def add_sitemap(self, site_map_path=None, comments=\"\"): \"\"\"add file path of sitemap to current user agent. Args: site_map_path (str): location of sitemap. Defaults to None. comments (str): any comments to include with sitemap path. Defaults to \"\". \"\"\" if not site_map_path: raise ValueError self.sitemaps.append(site_map_path) def remove_sitemap(self, site_map_path=None): \"\"\"remove a sitemap from current user agent. Args: site_map_path (str): sitemap file path to be removed. Defaults to None. \"\"\" if site_map_path in self.sitemaps: self.sitemaps -= [site_map_path] def disallow_pagination(self, prefix=\"/page/*\", comments=\"\"): \"\"\"Single function to disable pagination on a website using robots.txt file. Args: prefix (str, optional): prefix for pages (default - /page/). Defaults to \"/page/*\". comments (str, optional): human readable comments for inclusion. Defaults to \"\". \"\"\" self.add_disallow(disallow_item=prefix, comments=comments) def consolidate(self): \"\"\"consolidate all the information (allowed, disallowed, sitemaps) in single text string.\"\"\" self.content = f\"\\n\\nUser-agent: {self.user_agent_name}\\n\" if self.crawl_delay > 0: self.content = f\"\\nCrawl-delay: {self.crawl_delay}\\n\" if self.allowed: self.content += \"\\n# Allowed Patterns\\n\" self.content += \"\\n\".join([f\"Allow: {item}\" for item in self.allowed]) if self.disallowed: self.content += \"\\n\\n# Disallowed Patterns\\n\" self.content += \"\\n\".join([f\"Disallow: {item}\" for item in self.disallowed]) if self.sitemaps: self.content += \"\\n\\n# Site Maps\\n\" self.content += \"\\n\".join([f\"Sitemap: {item}\" for item in self.sitemaps]) __init__(ua_name='*', crawl_delay=0) \u00b6 Initialize UserAgent objet with a user-agent name and crawl delay varible. Parameters: Name Type Description Default ua_name str name of the user-agent. Defaults to \u201c*\u201d. '*' crawl_delay int crawl delay value for user agent/bots. Defaults to 0. 0 Source code in pyrobotstxt/__init__.py def __init__(self, ua_name=\"*\", crawl_delay=0): \"\"\"Initialize UserAgent objet with a user-agent name and crawl delay varible. Args: ua_name (str, optional): name of the user-agent. Defaults to \"*\". crawl_delay (int, optional): crawl delay value for user agent/bots. Defaults to 0. \"\"\" self.user_agent_name = ua_name self.crawl_delay = crawl_delay self.sitemaps = [] # lists of sitemap for current UserAgent self.allowed = [] # lists of Allowed Items for current UserAgent self.disallowed = [] # lists of Disallowed Items for current UserAgent self.content = \"\" # consolidate content for robots.txt file add_allow(allow_items, unique=True, comments='') \u00b6 Add allowed items/pages/slugs to current User Agent. Parameters: Name Type Description Default allow_items str , list single item or list of items allowed for current user agnet. required unique bool If True duplicate item stripped to single value. Defaults to True. True comments str Any comments for added value for human readability. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_allow(self, allow_items, unique=True, comments=\"\"): \"\"\"Add allowed items/pages/slugs to current User Agent. Args: allow_items (str, list): single item or list of items allowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(allow_items, str): allow_items = [allow_items] if not isinstance(allow_items, list): print(\"not supported\", type(allow_items)) # raise exception raise TypeError else: self.allowed += allow_items if unique: self.allowed = list(set(self.allowed)) add_disallow(disallow_items, unique=True, comments='') \u00b6 Add disallowed items/pages/slugs to current User Agent. Parameters: Name Type Description Default disallow_items str , list single item or list of items disallowed for current user agnet. required unique bool If True duplicate item stripped to single value. Defaults to True. True comments str Any comments for added value for human readability. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_disallow(self, disallow_items, unique=True, comments=\"\"): \"\"\"Add disallowed items/pages/slugs to current User Agent. Args: disallow_items (str, list): single item or list of items disallowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(disallow_items, str): disallow_items = [disallow_items] if not isinstance(disallow_items, list): print(\"not supported\", type(disallow_items)) # raise exception raise TypeError else: self.disallowed += disallow_items if unique: self.disallowed = list(set(self.disallowed)) add_sitemap(site_map_path=None, comments='') \u00b6 add file path of sitemap to current user agent. Parameters: Name Type Description Default site_map_path str location of sitemap. Defaults to None. None comments str any comments to include with sitemap path. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_sitemap(self, site_map_path=None, comments=\"\"): \"\"\"add file path of sitemap to current user agent. Args: site_map_path (str): location of sitemap. Defaults to None. comments (str): any comments to include with sitemap path. Defaults to \"\". \"\"\" if not site_map_path: raise ValueError self.sitemaps.append(site_map_path) consolidate() \u00b6 consolidate all the information (allowed, disallowed, sitemaps) in single text string. Source code in pyrobotstxt/__init__.py def consolidate(self): \"\"\"consolidate all the information (allowed, disallowed, sitemaps) in single text string.\"\"\" self.content = f\"\\n\\nUser-agent: {self.user_agent_name}\\n\" if self.crawl_delay > 0: self.content = f\"\\nCrawl-delay: {self.crawl_delay}\\n\" if self.allowed: self.content += \"\\n# Allowed Patterns\\n\" self.content += \"\\n\".join([f\"Allow: {item}\" for item in self.allowed]) if self.disallowed: self.content += \"\\n\\n# Disallowed Patterns\\n\" self.content += \"\\n\".join([f\"Disallow: {item}\" for item in self.disallowed]) if self.sitemaps: self.content += \"\\n\\n# Site Maps\\n\" self.content += \"\\n\".join([f\"Sitemap: {item}\" for item in self.sitemaps]) disallow_pagination(prefix='/page/*', comments='') \u00b6 Single function to disable pagination on a website using robots.txt file. Parameters: Name Type Description Default prefix str prefix for pages (default - /page/). Defaults to \u201c/page/*\u201d. '/page/*' comments str human readable comments for inclusion. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def disallow_pagination(self, prefix=\"/page/*\", comments=\"\"): \"\"\"Single function to disable pagination on a website using robots.txt file. Args: prefix (str, optional): prefix for pages (default - /page/). Defaults to \"/page/*\". comments (str, optional): human readable comments for inclusion. Defaults to \"\". \"\"\" self.add_disallow(disallow_item=prefix, comments=comments) remove_allow(allow_item) \u00b6 Remove any previously added allowed item from allowed list. Parameters: Name Type Description Default allow_item str , list item(s) to be removed. required Source code in pyrobotstxt/__init__.py def remove_allow(self, allow_item): \"\"\"Remove any previously added allowed item from allowed list. Args: allow_item (str, list): item(s) to be removed. \"\"\" if allow_item in self.allowed: self.allowed -= [allow_item] remove_disallow(disallow_item) \u00b6 Remove any previously added disallowed item from allowed list. Parameters: Name Type Description Default disallow_item str , list item(s) to be removed. required Source code in pyrobotstxt/__init__.py def remove_disallow(self, disallow_item): \"\"\"Remove any previously added disallowed item from allowed list. Args: disallow_item (str, list): item(s) to be removed. \"\"\" if disallow_item in self.disallowed: self.disallowed -= [disallow_item] remove_sitemap(site_map_path=None) \u00b6 remove a sitemap from current user agent. Parameters: Name Type Description Default site_map_path str sitemap file path to be removed. Defaults to None. None Source code in pyrobotstxt/__init__.py def remove_sitemap(self, site_map_path=None): \"\"\"remove a sitemap from current user agent. Args: site_map_path (str): sitemap file path to be removed. Defaults to None. \"\"\" if site_map_path in self.sitemaps: self.sitemaps -= [site_map_path]","title":"UserAgent"},{"location":"useragent/#useragent-class","text":"Source code in pyrobotstxt/__init__.py class UserAgent: def __init__(self, ua_name=\"*\", crawl_delay=0): \"\"\"Initialize UserAgent objet with a user-agent name and crawl delay varible. Args: ua_name (str, optional): name of the user-agent. Defaults to \"*\". crawl_delay (int, optional): crawl delay value for user agent/bots. Defaults to 0. \"\"\" self.user_agent_name = ua_name self.crawl_delay = crawl_delay self.sitemaps = [] # lists of sitemap for current UserAgent self.allowed = [] # lists of Allowed Items for current UserAgent self.disallowed = [] # lists of Disallowed Items for current UserAgent self.content = \"\" # consolidate content for robots.txt file def add_allow(self, allow_items, unique=True, comments=\"\"): \"\"\"Add allowed items/pages/slugs to current User Agent. Args: allow_items (str, list): single item or list of items allowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(allow_items, str): allow_items = [allow_items] if not isinstance(allow_items, list): print(\"not supported\", type(allow_items)) # raise exception raise TypeError else: self.allowed += allow_items if unique: self.allowed = list(set(self.allowed)) def remove_allow(self, allow_item): \"\"\"Remove any previously added allowed item from allowed list. Args: allow_item (str, list): item(s) to be removed. \"\"\" if allow_item in self.allowed: self.allowed -= [allow_item] def add_disallow(self, disallow_items, unique=True, comments=\"\"): \"\"\"Add disallowed items/pages/slugs to current User Agent. Args: disallow_items (str, list): single item or list of items disallowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(disallow_items, str): disallow_items = [disallow_items] if not isinstance(disallow_items, list): print(\"not supported\", type(disallow_items)) # raise exception raise TypeError else: self.disallowed += disallow_items if unique: self.disallowed = list(set(self.disallowed)) def remove_disallow(self, disallow_item): \"\"\"Remove any previously added disallowed item from allowed list. Args: disallow_item (str, list): item(s) to be removed. \"\"\" if disallow_item in self.disallowed: self.disallowed -= [disallow_item] def add_sitemap(self, site_map_path=None, comments=\"\"): \"\"\"add file path of sitemap to current user agent. Args: site_map_path (str): location of sitemap. Defaults to None. comments (str): any comments to include with sitemap path. Defaults to \"\". \"\"\" if not site_map_path: raise ValueError self.sitemaps.append(site_map_path) def remove_sitemap(self, site_map_path=None): \"\"\"remove a sitemap from current user agent. Args: site_map_path (str): sitemap file path to be removed. Defaults to None. \"\"\" if site_map_path in self.sitemaps: self.sitemaps -= [site_map_path] def disallow_pagination(self, prefix=\"/page/*\", comments=\"\"): \"\"\"Single function to disable pagination on a website using robots.txt file. Args: prefix (str, optional): prefix for pages (default - /page/). Defaults to \"/page/*\". comments (str, optional): human readable comments for inclusion. Defaults to \"\". \"\"\" self.add_disallow(disallow_item=prefix, comments=comments) def consolidate(self): \"\"\"consolidate all the information (allowed, disallowed, sitemaps) in single text string.\"\"\" self.content = f\"\\n\\nUser-agent: {self.user_agent_name}\\n\" if self.crawl_delay > 0: self.content = f\"\\nCrawl-delay: {self.crawl_delay}\\n\" if self.allowed: self.content += \"\\n# Allowed Patterns\\n\" self.content += \"\\n\".join([f\"Allow: {item}\" for item in self.allowed]) if self.disallowed: self.content += \"\\n\\n# Disallowed Patterns\\n\" self.content += \"\\n\".join([f\"Disallow: {item}\" for item in self.disallowed]) if self.sitemaps: self.content += \"\\n\\n# Site Maps\\n\" self.content += \"\\n\".join([f\"Sitemap: {item}\" for item in self.sitemaps])","title":"UserAgent Class"},{"location":"useragent/#pyrobotstxt.UserAgent.__init__","text":"Initialize UserAgent objet with a user-agent name and crawl delay varible. Parameters: Name Type Description Default ua_name str name of the user-agent. Defaults to \u201c*\u201d. '*' crawl_delay int crawl delay value for user agent/bots. Defaults to 0. 0 Source code in pyrobotstxt/__init__.py def __init__(self, ua_name=\"*\", crawl_delay=0): \"\"\"Initialize UserAgent objet with a user-agent name and crawl delay varible. Args: ua_name (str, optional): name of the user-agent. Defaults to \"*\". crawl_delay (int, optional): crawl delay value for user agent/bots. Defaults to 0. \"\"\" self.user_agent_name = ua_name self.crawl_delay = crawl_delay self.sitemaps = [] # lists of sitemap for current UserAgent self.allowed = [] # lists of Allowed Items for current UserAgent self.disallowed = [] # lists of Disallowed Items for current UserAgent self.content = \"\" # consolidate content for robots.txt file","title":"__init__()"},{"location":"useragent/#pyrobotstxt.UserAgent.add_allow","text":"Add allowed items/pages/slugs to current User Agent. Parameters: Name Type Description Default allow_items str , list single item or list of items allowed for current user agnet. required unique bool If True duplicate item stripped to single value. Defaults to True. True comments str Any comments for added value for human readability. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_allow(self, allow_items, unique=True, comments=\"\"): \"\"\"Add allowed items/pages/slugs to current User Agent. Args: allow_items (str, list): single item or list of items allowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(allow_items, str): allow_items = [allow_items] if not isinstance(allow_items, list): print(\"not supported\", type(allow_items)) # raise exception raise TypeError else: self.allowed += allow_items if unique: self.allowed = list(set(self.allowed))","title":"add_allow()"},{"location":"useragent/#pyrobotstxt.UserAgent.add_disallow","text":"Add disallowed items/pages/slugs to current User Agent. Parameters: Name Type Description Default disallow_items str , list single item or list of items disallowed for current user agnet. required unique bool If True duplicate item stripped to single value. Defaults to True. True comments str Any comments for added value for human readability. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_disallow(self, disallow_items, unique=True, comments=\"\"): \"\"\"Add disallowed items/pages/slugs to current User Agent. Args: disallow_items (str, list): single item or list of items disallowed for current user agnet. unique (bool, optional): If True duplicate item stripped to single value. Defaults to True. comments (str, optional): Any comments for added value for human readability. Defaults to \"\". \"\"\" if isinstance(disallow_items, str): disallow_items = [disallow_items] if not isinstance(disallow_items, list): print(\"not supported\", type(disallow_items)) # raise exception raise TypeError else: self.disallowed += disallow_items if unique: self.disallowed = list(set(self.disallowed))","title":"add_disallow()"},{"location":"useragent/#pyrobotstxt.UserAgent.add_sitemap","text":"add file path of sitemap to current user agent. Parameters: Name Type Description Default site_map_path str location of sitemap. Defaults to None. None comments str any comments to include with sitemap path. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def add_sitemap(self, site_map_path=None, comments=\"\"): \"\"\"add file path of sitemap to current user agent. Args: site_map_path (str): location of sitemap. Defaults to None. comments (str): any comments to include with sitemap path. Defaults to \"\". \"\"\" if not site_map_path: raise ValueError self.sitemaps.append(site_map_path)","title":"add_sitemap()"},{"location":"useragent/#pyrobotstxt.UserAgent.consolidate","text":"consolidate all the information (allowed, disallowed, sitemaps) in single text string. Source code in pyrobotstxt/__init__.py def consolidate(self): \"\"\"consolidate all the information (allowed, disallowed, sitemaps) in single text string.\"\"\" self.content = f\"\\n\\nUser-agent: {self.user_agent_name}\\n\" if self.crawl_delay > 0: self.content = f\"\\nCrawl-delay: {self.crawl_delay}\\n\" if self.allowed: self.content += \"\\n# Allowed Patterns\\n\" self.content += \"\\n\".join([f\"Allow: {item}\" for item in self.allowed]) if self.disallowed: self.content += \"\\n\\n# Disallowed Patterns\\n\" self.content += \"\\n\".join([f\"Disallow: {item}\" for item in self.disallowed]) if self.sitemaps: self.content += \"\\n\\n# Site Maps\\n\" self.content += \"\\n\".join([f\"Sitemap: {item}\" for item in self.sitemaps])","title":"consolidate()"},{"location":"useragent/#pyrobotstxt.UserAgent.disallow_pagination","text":"Single function to disable pagination on a website using robots.txt file. Parameters: Name Type Description Default prefix str prefix for pages (default - /page/). Defaults to \u201c/page/*\u201d. '/page/*' comments str human readable comments for inclusion. Defaults to \u201c\u201d. '' Source code in pyrobotstxt/__init__.py def disallow_pagination(self, prefix=\"/page/*\", comments=\"\"): \"\"\"Single function to disable pagination on a website using robots.txt file. Args: prefix (str, optional): prefix for pages (default - /page/). Defaults to \"/page/*\". comments (str, optional): human readable comments for inclusion. Defaults to \"\". \"\"\" self.add_disallow(disallow_item=prefix, comments=comments)","title":"disallow_pagination()"},{"location":"useragent/#pyrobotstxt.UserAgent.remove_allow","text":"Remove any previously added allowed item from allowed list. Parameters: Name Type Description Default allow_item str , list item(s) to be removed. required Source code in pyrobotstxt/__init__.py def remove_allow(self, allow_item): \"\"\"Remove any previously added allowed item from allowed list. Args: allow_item (str, list): item(s) to be removed. \"\"\" if allow_item in self.allowed: self.allowed -= [allow_item]","title":"remove_allow()"},{"location":"useragent/#pyrobotstxt.UserAgent.remove_disallow","text":"Remove any previously added disallowed item from allowed list. Parameters: Name Type Description Default disallow_item str , list item(s) to be removed. required Source code in pyrobotstxt/__init__.py def remove_disallow(self, disallow_item): \"\"\"Remove any previously added disallowed item from allowed list. Args: disallow_item (str, list): item(s) to be removed. \"\"\" if disallow_item in self.disallowed: self.disallowed -= [disallow_item]","title":"remove_disallow()"},{"location":"useragent/#pyrobotstxt.UserAgent.remove_sitemap","text":"remove a sitemap from current user agent. Parameters: Name Type Description Default site_map_path str sitemap file path to be removed. Defaults to None. None Source code in pyrobotstxt/__init__.py def remove_sitemap(self, site_map_path=None): \"\"\"remove a sitemap from current user agent. Args: site_map_path (str): sitemap file path to be removed. Defaults to None. \"\"\" if site_map_path in self.sitemaps: self.sitemaps -= [site_map_path]","title":"remove_sitemap()"}]}